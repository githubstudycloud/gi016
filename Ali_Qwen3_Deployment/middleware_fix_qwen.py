import os
import json
import re
import uvicorn
import httpx
import asyncio
from fastapi import FastAPI, Request
from fastapi.responses import JSONResponse

# ================= ç”¨æˆ·é…ç½®åŒºåŸŸ =================
# æ‚¨çš„ vLLM æœåŠ¡åœ°å€ (æ ¹æ®æ‚¨çš„æè¿°ï¼Œç«¯å£æ˜¯ 8001)
VLLM_API_BASE = "http://localhost:8001/v1"

# ä¸­é—´ä»¶ç›‘å¬ç«¯å£ (Claude Code å°†è¿æ¥è¿™ä¸ªç«¯å£)
# ä¿æŒ 4000 ä¸å˜ï¼Œè¿™æ ·æ–¹ä¾¿é…ç½®
PORT = 4000

# è‡ªåŠ¨è·å–æ¨¡å‹åç§°
# å¦‚æœè®¾ç½®ä¸º Noneï¼Œè„šæœ¬å¯åŠ¨æ—¶ä¼šè‡ªåŠ¨å» vLLM æŸ¥è¯¢æ­£åœ¨è¿è¡Œçš„æ¨¡å‹åç§°
ACTUAL_MODEL_NAME = None 
# ===========================================

app = FastAPI()
client = httpx.AsyncClient(timeout=600.0)

async def get_running_model_name():
    """è‡ªåŠ¨ä» vLLM è·å–å½“å‰è¿è¡Œçš„æ¨¡å‹åç§°"""
    global ACTUAL_MODEL_NAME
    if ACTUAL_MODEL_NAME:
        return ACTUAL_MODEL_NAME
    
    try:
        print(f"ğŸ” æ­£åœ¨è¿æ¥ {VLLM_API_BASE}/models è·å–æ¨¡å‹åç§°...")
        resp = await client.get(f"{VLLM_API_BASE}/models")
        if resp.status_code == 200:
            data = resp.json()
            # è·å–ç¬¬ä¸€ä¸ªæ¨¡å‹çš„ ID
            model_id = data["data"][0]["id"]
            print(f"âœ… æ£€æµ‹åˆ° vLLM æ­£åœ¨è¿è¡Œæ¨¡å‹: {model_id}")
            ACTUAL_MODEL_NAME = model_id
            return model_id
    except Exception as e:
        print(f"âš ï¸ æ— æ³•è‡ªåŠ¨è·å–æ¨¡å‹åç§°: {e}")
        return "Qwen/Qwen3-235B-A22B-Instruct" # é»˜è®¤å›é€€å€¼

def parse_hermes_xml(content):
    """
    å°è¯•ä»æ–‡æœ¬ä¸­æå– Hermes é£æ ¼çš„ <tool_code> XML å¹¶è½¬æ¢ä¸º OpenAI æ ¼å¼çš„ tool_calls
    """
    tool_calls = []
    
    # åŒ¹é… <tool_code>...</tool_code>
    pattern = r"<tool_code>\s*(.*?)\s*</tool_code>"
    matches = re.findall(pattern, content, re.DOTALL)
    
    for i, code_str in enumerate(matches):
        try:
            # æœ‰æ—¶å€™æ¨¡å‹ä¼šåœ¨ JSON å¤–é¢åŒ…ä¸€å±‚ markdown ä»£ç å—ï¼Œå¦‚ ```json ... ```
            # éœ€è¦æ¸…æ´—æ‰
            clean_json = re.sub(r"^```json\s*|\s*```$", "", code_str.strip(), flags=re.IGNORECASE)
            
            tool_call_data = json.loads(clean_json)
            
            tool_calls.append({
                "id": f"call_{i}_{os.urandom(4).hex()}",
                "type": "function",
                "function": {
                    "name": tool_call_data.get("name"),
                    "arguments": json.dumps(tool_call_data.get("arguments", {}))
                }
            })
        except json.JSONDecodeError:
            print(f"âš ï¸ è§£æå·¥å…·è°ƒç”¨ JSON å¤±è´¥: {code_str}")
            continue
            
    return tool_calls

@app.post("/v1/chat/completions")
@app.post("/chat/completions")
async def proxy_chat(request: Request):
    try:
        # 1. è·å–å®¢æˆ·ç«¯ (Claude Code) å‘é€çš„è¯·æ±‚
        body = await request.json()
        
        # 2. ä¿®æ­£æ¨¡å‹åç§°
        # Claude Code ä¼šå‘é€ "claude-3-5-sonnet..."ï¼Œæˆ‘ä»¬éœ€è¦æŠŠå®ƒæ”¹æˆ vLLM å®é™…è¿è¡Œçš„æ¨¡å‹å
        target_model = await get_running_model_name()
        if target_model:
            body["model"] = target_model
        
        # 3. æ¸…ç†ä¸æ”¯æŒçš„å‚æ•° (é˜²æ­¢ 400 é”™è¯¯)
        if "metadata" in body:
            del body["metadata"]
        
        # 4. å¼ºåˆ¶å…³é—­æµå¼è¾“å‡º
        # ä¸ºäº†èƒ½å®Œæ•´è§£æ XMLï¼Œæˆ‘ä»¬å¿…é¡»æ‹¦æˆªæ•´ä¸ªå“åº”ï¼Œä¸èƒ½æµå¼ä¼ è¾“
        original_stream = body.get("stream", False)
        body["stream"] = False 
        
        # 5. è½¬å‘è¯·æ±‚ç»™ vLLM (ç«¯å£ 8001)
        response = await client.post(
            f"{VLLM_API_BASE}/chat/completions",
            json=body,
            headers={"Authorization": "Bearer sk-empty"}
        )
        
        if response.status_code != 200:
            print(f"âŒ vLLM è¿”å›é”™è¯¯: {response.status_code} - {response.text}")
            return JSONResponse(content=response.json(), status_code=response.status_code)
            
        result = response.json()
        
        # 6. æ ¸å¿ƒé€»è¾‘ï¼šæ£€æŸ¥å¹¶ä¿®å¤å·¥å…·è°ƒç”¨
        if "choices" in result and len(result["choices"]) > 0:
            choice = result["choices"][0]
            message = choice["message"]
            content = message.get("content", "") or ""
            
            # å¦‚æœå†…å®¹é‡ŒåŒ…å« <tool_code>ï¼Œè¯´æ˜æ¨¡å‹æƒ³è°ƒç”¨å·¥å…·ä½† vLLM æ²¡è§£æå‡ºæ¥
            if "<tool_code>" in content:
                print(f"ğŸ› ï¸ æ£€æµ‹åˆ°åŸå§‹ XMLï¼Œæ­£åœ¨è¿›è¡Œæ ¼å¼è½¬æ¢...")
                extracted_tools = parse_hermes_xml(content)
                
                if extracted_tools:
                    print(f"âœ… æˆåŠŸæå– {len(extracted_tools)} ä¸ªå·¥å…·è°ƒç”¨")
                    message["tool_calls"] = extracted_tools
                    # æŒ‰ç…§ OpenAI è§„èŒƒï¼Œå¦‚æœæ˜¯å·¥å…·è°ƒç”¨ï¼Œcontent é€šå¸¸ä¸º null
                    # æˆ–è€…ä¿ç•™ <think> æ ‡ç­¾çš„å†…å®¹
                    message["content"] = None 
                    choice["finish_reason"] = "tool_calls"
        
        return result

    except Exception as e:
        print(f"âŒ ä»£ç†å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
        return JSONResponse(content={"error": str(e)}, status_code=500)

@app.on_event("startup")
async def startup_event():
    # å¯åŠ¨æ—¶é¢„å…ˆè·å–ä¸€æ¬¡æ¨¡å‹åç§°
    await get_running_model_name()

if __name__ == "__main__":
    print(f"ğŸš€ Qwen3 ä¸“ç”¨ä¿®å¤ä¸­é—´ä»¶å·²å¯åŠ¨")
    print(f"ğŸ“¡ è¿æ¥ vLLM åœ°å€: {VLLM_API_BASE}")
    print(f"ğŸ‘‚ æœ¬åœ°ç›‘å¬ç«¯å£: {PORT}")
    print(f"ğŸ‘‰ è¯·é…ç½® Claude Code ä½¿ç”¨: http://localhost:{PORT}")
    uvicorn.run(app, host="0.0.0.0", port=PORT)
