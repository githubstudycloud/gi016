import os
import json
import re
import uvicorn
import httpx
import time
from fastapi import FastAPI, Request
from fastapi.responses import JSONResponse

# ================= ç”¨æˆ·é…ç½®åŒºåŸŸ =================
# æ‚¨çš„ vLLM æœåŠ¡åœ°å€
VLLM_API_BASE = "http://localhost:8001/v1"

# ä¸­é—´ä»¶ç›‘å¬ç«¯å£
PORT = 4000

# æ‚¨çš„è‡ªå®šä¹‰æ¨¡å‹åç§° (åœ¨è¿™é‡Œå†™æ­»)
TARGET_MODEL_NAME = "Qwen/Qwen3-235B-A22B-Instruct" 

# vLLM çš„ API Key (åœ¨è¿™é‡Œå†™æ­»)
VLLM_API_KEY = "empty"

# ä¸Šä¸‹æ–‡é™åˆ¶ (80k)
MAX_CONTEXT_TOKENS = 80000 
# ===========================================

app = FastAPI()
client = httpx.AsyncClient(timeout=600.0)

def estimate_tokens(text):
    return len(text) // 3

def parse_hermes_xml(content):
    """å°è¯•ä»æ–‡æœ¬ä¸­æå– Hermes é£æ ¼çš„ <tool_code> XML"""
    tool_calls = []
    pattern = r"<tool_code>\s*(.*?)\s*</tool_code>"
    matches = re.findall(pattern, content, re.DOTALL)
    
    for i, code_str in enumerate(matches):
        try:
            clean_json = re.sub(r"^```json\s*|\s*```$", "", code_str.strip(), flags=re.IGNORECASE)
            tool_call_data = json.loads(clean_json)
            tool_calls.append({
                "id": f"call_{i}_{os.urandom(4).hex()}",
                "type": "function",
                "function": {
                    "name": tool_call_data.get("name"),
                    "arguments": json.dumps(tool_call_data.get("arguments", {}))
                }
            })
        except json.JSONDecodeError:
            print(f"âš ï¸ è§£æå·¥å…·è°ƒç”¨ JSON å¤±è´¥: {code_str}")
            continue
    return tool_calls

def convert_claude_messages_to_openai(claude_body):
    """å°† Claude æ ¼å¼çš„ messages è¯·æ±‚è½¬æ¢ä¸º OpenAI æ ¼å¼"""
    openai_messages = []
    
    # 1. å¤„ç† system prompt
    if "system" in claude_body:
        openai_messages.append({
            "role": "system",
            "content": claude_body["system"]
        })
        
    # 2. å¤„ç† messages åˆ—è¡¨
    for msg in claude_body.get("messages", []):
        role = msg["role"]
        content = msg["content"]
        
        # Claude çš„ content å¯èƒ½æ˜¯åˆ—è¡¨ï¼ˆåŒ…å« text æˆ– imageï¼‰
        if isinstance(content, list):
            new_content = ""
            for part in content:
                if part.get("type") == "text":
                    new_content += part.get("text", "")
                # æš‚æ—¶å¿½ç•¥ imageï¼Œå› ä¸º vLLM OpenAI æ¥å£é€šå¸¸éœ€è¦ URL æˆ– base64
                # å¦‚æœéœ€è¦æ”¯æŒå¤šæ¨¡æ€ï¼Œè¿™é‡Œéœ€è¦æ›´å¤æ‚çš„è½¬æ¢
            
            openai_messages.append({"role": role, "content": new_content})
        else:
            openai_messages.append({"role": role, "content": content})
            
    # 3. å¤„ç† tools
    tools = []
    if "tools" in claude_body:
        for tool in claude_body["tools"]:
            tools.append({
                "type": "function",
                "function": {
                    "name": tool["name"],
                    "description": tool.get("description", ""),
                    "parameters": tool["input_schema"] # Claude input_schema -> OpenAI parameters
                }
            })
            
    return openai_messages, tools

def convert_openai_response_to_claude(openai_resp):
    """å°† OpenAI æ ¼å¼çš„å“åº”è½¬æ¢ä¸º Claude æ ¼å¼"""
    choice = openai_resp["choices"][0]
    message = choice["message"]
    
    claude_content = []
    stop_reason = "end_turn"
    
    # 1. å¤„ç†æ–‡æœ¬å†…å®¹
    if message.get("content"):
        claude_content.append({
            "type": "text",
            "text": message["content"]
        })
        
    # 2. å¤„ç†å·¥å…·è°ƒç”¨
    if message.get("tool_calls"):
        stop_reason = "tool_use"
        for tool_call in message["tool_calls"]:
            claude_content.append({
                "type": "tool_use",
                "id": tool_call["id"],
                "name": tool_call["function"]["name"],
                "input": json.loads(tool_call["function"]["arguments"])
            })
            
    return {
        "id": openai_resp["id"],
        "type": "message",
        "role": "assistant",
        "content": claude_content,
        "model": TARGET_MODEL_NAME,
        "stop_reason": stop_reason,
        "stop_sequence": None,
        "usage": {
            "input_tokens": openai_resp["usage"]["prompt_tokens"],
            "output_tokens": openai_resp["usage"]["completion_tokens"]
        }
    }

# æ‹¦æˆª Claude çš„æ ¸å¿ƒè·¯ç”±
@app.post("/v1/messages")
@app.post("/messages")
async def proxy_claude_messages(request: Request):
    try:
        body = await request.json()
        print("ğŸ“¨ æ”¶åˆ° Claude åè®®è¯·æ±‚ (/v1/messages)")
        
        # 1. ä¼°ç®— Token (ç®€å•ä¿æŠ¤)
        # è¿™é‡Œåªä¼°ç®— messages é‡Œçš„æ–‡æœ¬é•¿åº¦
        total_chars = 0
        if "system" in body:
             total_chars += len(body["system"])
        for msg in body.get("messages", []):
            if isinstance(msg["content"], str):
                total_chars += len(msg["content"])
        
        if (total_chars // 3) > MAX_CONTEXT_TOKENS:
             return JSONResponse(
                content={
                    "type": "error",
                    "error": {
                        "type": "invalid_request_error",
                        "message": f"Context limit reached! Please run /compact."
                    }
                },
                status_code=400
            )

        # 2. åè®®è½¬æ¢: Claude -> OpenAI
        openai_messages, tools = convert_claude_messages_to_openai(body)
        
        openai_req = {
            "model": TARGET_MODEL_NAME, # ä½¿ç”¨ç¡¬ç¼–ç çš„æ¨¡å‹å
            "messages": openai_messages,
            "max_tokens": body.get("max_tokens", 4096),
            "temperature": body.get("temperature", 0.7),
            "stream": False # å¼ºåˆ¶å…³é—­æµå¼ï¼Œä»¥ä¾¿ä¿®å¤å·¥å…·è°ƒç”¨
        }
        
        if tools:
            openai_req["tools"] = tools
            openai_req["tool_choice"] = "auto"

        # 3. å‘é€ç»™ vLLM (OpenAI æ¥å£)
        print(f"ğŸš€ è½¬å‘ç»™ vLLM (æ¨¡å‹: {TARGET_MODEL_NAME})...")
        response = await client.post(
            f"{VLLM_API_BASE}/chat/completions",
            json=openai_req,
            headers={"Authorization": f"Bearer {VLLM_API_KEY}"},
            timeout=600.0
        )
        
        if response.status_code != 200:
            print(f"âŒ vLLM æŠ¥é”™: {response.text}")
            return JSONResponse(content={"error": "vLLM error"}, status_code=response.status_code)
            
        openai_result = response.json()
        
        # 4. æ£€æŸ¥å¹¶ä¿®å¤ XML å·¥å…·è°ƒç”¨
        choice = openai_result["choices"][0]
        content = choice["message"].get("content", "") or ""
        
        if "<tool_code>" in content:
            print(f"ğŸ› ï¸ æ•è·åˆ° XML å·¥å…·è°ƒç”¨ï¼Œæ­£åœ¨ä¿®å¤...")
            extracted_tools = parse_hermes_xml(content)
            if extracted_tools:
                choice["message"]["tool_calls"] = extracted_tools
                # Claude åè®®å…è®¸ tool_use å’Œ text åŒæ—¶å­˜åœ¨ï¼Œæ‰€ä»¥ä¸éœ€è¦æ¸…ç©º content
                # ä½†ä¸ºäº†æ•´æ´ï¼Œå¦‚æœåªæœ‰å·¥å…·è°ƒç”¨ï¼Œæˆ‘ä»¬å¯ä»¥æŠŠ XML ä» content é‡Œå»æ‰
                # è¿™é‡Œç®€å•èµ·è§ï¼Œä¿ç•™ content (ä½œä¸ºæ€è€ƒè¿‡ç¨‹) ä¹Ÿæ˜¯å¯ä»¥çš„
        
        # 5. åè®®è½¬æ¢: OpenAI -> Claude
        claude_response = convert_openai_response_to_claude(openai_result)
        
        print("âœ… å“åº”æˆåŠŸè¿”å›")
        return JSONResponse(content=claude_response)

    except Exception as e:
        print(f"âŒ ä¸¥é‡é”™è¯¯: {e}")
        return JSONResponse(content={"error": str(e)}, status_code=500)

if __name__ == "__main__":
    print(f"ğŸš€ Claude åè®®å…¼å®¹å±‚å·²å¯åŠ¨")
    print(f"ğŸ¯ ç›®æ ‡æ¨¡å‹: {TARGET_MODEL_NAME}")
    print(f"ğŸ”‘ API Key: {VLLM_API_KEY}")
    print(f"ğŸ“¡ ç›‘å¬ç«¯å£: {PORT} (è¯·é…ç½® Claude Code Base URL ä¸º http://localhost:{PORT})")
    uvicorn.run(app, host="0.0.0.0", port=PORT)
