# é˜¿é‡Œ Qwen3-235B å¤§æ¨¡å‹éƒ¨ç½²ä¸ Claude Code é›†æˆç»ˆææŒ‡å— (æ–°æ‰‹ç‰ˆ)

## ğŸ“– 1. é¡¹ç›®èƒŒæ™¯ä¸ç›®æ ‡

æœ¬æŒ‡å—å°†æ‰‹æŠŠæ‰‹æ•™ä½ å¦‚ä½•åœ¨æœ¬åœ°ï¼ˆæˆ–äº‘æœåŠ¡å™¨ï¼‰éƒ¨ç½²é˜¿é‡Œæœ€å¼ºå¤§çš„ **Qwen3-235B-A22B-Instruct** å¤§æ¨¡å‹ï¼Œå¹¶è®©å®ƒèƒ½å¤Ÿè¢« **Claude Code** ç­‰å·¥å…·è°ƒç”¨ã€‚

### ä¸ºä»€ä¹ˆéœ€è¦è¿™ä¸ªæ•™ç¨‹ï¼Ÿ
ä½ å¯èƒ½é‡åˆ°äº†è¿™æ ·çš„é—®é¢˜ï¼š
1.  æƒ³ç”¨å¼ºå¤§çš„ Qwen3 æ¨¡å‹ï¼Œä½† Claude Code é»˜è®¤åªæ”¯æŒ Claude ç³»åˆ—æ¨¡å‹ã€‚
2.  å°è¯•è¿æ¥æ—¶ï¼Œæ€»æ˜¯æŠ¥ **400 Bad Request** é”™è¯¯ã€‚
3.  æ¨¡å‹è·‘èµ·æ¥äº†ï¼Œä½†æ— æ³•è‡ªåŠ¨è°ƒç”¨å·¥å…·ï¼ˆTool Calling å¤±è´¥ï¼‰ã€‚

### æ ¸å¿ƒè§£å†³æ–¹æ¡ˆæ¶æ„
æˆ‘ä»¬å°†ä½¿ç”¨ä»¥ä¸‹ç»„ä»¶æ­å»ºä¸€å¥—å®Œæ•´çš„æœåŠ¡ï¼š

*   **vLLM** (æ¨ç†å¼•æ“): ç›¸å½“äºæ±½è½¦çš„â€œå‘åŠ¨æœºâ€ã€‚å®ƒè´Ÿè´£åŠ è½½åºå¤§çš„ Qwen3 æ¨¡å‹ï¼Œå¹¶æä¾›é«˜æ€§èƒ½çš„æ¨ç†æœåŠ¡ã€‚vLLM æ˜¯ç›®å‰è¿è¡Œ MoEï¼ˆæ··åˆä¸“å®¶ï¼‰æ¨¡å‹æ•ˆç‡æœ€é«˜çš„å·¥å…·ä¹‹ä¸€ã€‚
*   **LiteLLM** (åè®®ç½‘å…³): ç›¸å½“äºâ€œç¿»è¯‘å®˜â€ã€‚Claude Code è¯´çš„æ˜¯ Anthropic è¯­è¨€ï¼ˆåè®®ï¼‰ï¼Œè€Œ vLLM è¯´çš„æ˜¯ OpenAI è¯­è¨€ã€‚LiteLLM è´Ÿè´£åœ¨ä¸­é—´è¿›è¡Œç¿»è¯‘ï¼Œå¹¶ä¿®æ­£å¯¼è‡´æŠ¥é”™çš„å‚æ•°ã€‚

---

## ğŸ› ï¸ 2. ç¯å¢ƒå‡†å¤‡

åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿ä½ çš„æœºå™¨æ»¡è¶³ä»¥ä¸‹æ¡ä»¶ï¼š
*   **æ“ä½œç³»ç»Ÿ**: Linux (æ¨è Ubuntu 22.04) æˆ– Windows (WSL2)ã€‚
*   **æ˜¾å¡ (GPU)**: å¿…é¡»æ˜¯ NVIDIA æ˜¾å¡ã€‚ç”±äº Qwen3-235B å³ä½¿æ˜¯ FP8 é‡åŒ–ç‰ˆæœ¬ä¹Ÿå¾ˆå¤§ï¼Œå»ºè®®æ˜¾å­˜æ€»é‡ **> 250GB** (ä¾‹å¦‚ 4å¼  H20 æˆ– 4å¼  A100)ã€‚
*   **Python**: ç‰ˆæœ¬ 3.10 æˆ–æ›´é«˜ã€‚

### å®‰è£…å¿…è¦è½¯ä»¶
æ‰“å¼€ç»ˆç«¯ï¼Œæ‰§è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£…ä¾èµ–ï¼š

```bash
# 1. å®‰è£… vLLM (ç”¨äºè¿è¡Œæ¨¡å‹)
# æ³¨æ„ï¼švLLM è¿­ä»£å¾ˆå¿«ï¼Œå»ºè®®å®‰è£…æœ€æ–°ç‰ˆ
pip install vllm>=0.7.0

# 2. å®‰è£… LiteLLM (ç”¨äºåè®®è½¬æ¢)
# [proxy] é€‰é¡¹ä¼šå®‰è£…å¯åŠ¨æœåŠ¡å™¨æ‰€éœ€çš„é¢å¤–ä¾èµ–
pip install 'litellm[proxy]'
```

---

## ğŸš€ 3. ç¬¬ä¸€æ­¥ï¼šå¯åŠ¨ vLLM æ¨ç†æœåŠ¡

è¿™æ˜¯æœ€å…³é”®çš„ä¸€æ­¥ï¼Œæˆ‘ä»¬éœ€è¦å¯åŠ¨æ¨¡å‹å¹¶å¼€å¯â€œå·¥å…·è°ƒç”¨â€æ”¯æŒã€‚

### å¯åŠ¨å‘½ä»¤è¯¦è§£
è¯·åœ¨ç»ˆç«¯è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼ˆæˆ–ä¿å­˜ä¸º `start_vllm.ps1` / `start_vllm.sh` è¿è¡Œï¼‰ï¼š

```bash
python -m vllm.entrypoints.openai.api_server \
    --model Qwen/Qwen3-235B-A22B-Instruct \
    --served-model-name Qwen/Qwen3-235B-A22B-Instruct \
    --trust-remote-code \
    --tensor-parallel-size 4 \
    --gpu-memory-utilization 0.95 \
    --max-model-len 32768 \
    --enable-auto-tool-choice \
    --quantization fp8 \
    --port 8000
```

### å‚æ•°é€è¡Œä»¥æ­¤è§£é‡Š (æ–°æ‰‹å¿…è¯»)
*   `--model`: æŒ‡å®šæ¨¡å‹è·¯å¾„ã€‚vLLM ä¼šè‡ªåŠ¨ä» HuggingFace ä¸‹è½½ã€‚
*   `--trust-remote-code`: å…è®¸æ‰§è¡Œæ¨¡å‹ä»“åº“é‡Œçš„ä»£ç ï¼ˆQwen æ¨¡å‹å¿…é¡»é¡¹ï¼‰ã€‚
*   `--tensor-parallel-size 4`: **é‡è¦ï¼** è¡¨ç¤ºä½ ç”¨å‡ å¼ æ˜¾å¡æ¥è·‘è¿™ä¸ªæ¨¡å‹ã€‚å¦‚æœä½ æœ‰ 4 å¼ å¡ï¼Œå°±å¡« 4ã€‚
*   `--max-model-len 32768`: è®¾ç½®ä¸Šä¸‹æ–‡é•¿åº¦ã€‚Qwen3 æ”¯æŒå¾ˆé•¿ï¼Œä½†æ˜¾å­˜æœ‰é™ï¼Œè®¾å¤ªå¤§å®¹æ˜“çˆ†æ˜¾å­˜ï¼ˆOOMï¼‰ã€‚
*   `--enable-auto-tool-choice`: **å…³é”®å‚æ•°ï¼** å¼€å¯è¿™ä¸ªï¼Œæ¨¡å‹æ‰èƒ½æ ¹æ®ç”¨æˆ·çš„é—®é¢˜è‡ªåŠ¨å†³å®šâ€œæˆ‘æ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·â€ã€‚æ²¡æœ‰å®ƒï¼Œå·¥å…·è°ƒç”¨å°†å¤±æ•ˆã€‚
*   `--quantization fp8`: ä½¿ç”¨ FP8 é‡åŒ–åŠ è½½ï¼Œå¯ä»¥èŠ‚çœä¸€åŠæ˜¾å­˜å¹¶æå‡é€Ÿåº¦ã€‚
*   `--port 8000`: vLLM æœåŠ¡ç›‘å¬çš„ç«¯å£ã€‚

å¯åŠ¨æˆåŠŸåï¼Œä½ ä¼šçœ‹åˆ°ç±»ä¼¼ `Uvicorn running on http://0.0.0.0:8000` çš„æç¤ºã€‚

---

## ğŸŒ‰ 4. ç¬¬äºŒæ­¥ï¼šé…ç½® LiteLLM (è§£å†³ 400 é”™è¯¯)

ç›´æ¥ç”¨ Claude Code è¿æ¥ vLLM ä¼šæŠ¥ 400 é”™è¯¯ï¼Œå› ä¸º Claude Code ä¼šå‘é€ä¸€äº› vLLM çœ‹ä¸æ‡‚çš„å‚æ•°ï¼ˆæ¯”å¦‚ `tool_choice` çš„æŸäº›ç‰¹æ®Šç»“æ„ï¼‰ã€‚æˆ‘ä»¬éœ€è¦é…ç½® LiteLLM æ¥â€œæ¸…æ´—â€è¿™äº›è¯·æ±‚ã€‚

### åˆ›å»ºé…ç½®æ–‡ä»¶
åœ¨å½“å‰ç›®å½•ä¸‹åˆ›å»ºä¸€ä¸ªåä¸º `litellm_config.yaml` çš„æ–‡ä»¶ï¼Œå†…å®¹å¦‚ä¸‹ï¼š

```yaml
model_list:
  # ä¼ªè£…ç­–ç•¥ï¼šæˆ‘ä»¬å°†æœ¬åœ°çš„ Qwen æ¨¡å‹ä¼ªè£…æˆ Claude 3.5 Sonnet
  # è¿™æ ·å®¢æˆ·ç«¯å°±ä¼šæ”¾å¿ƒåœ°å‘é€è¯·æ±‚
  - model_name: claude-3-5-sonnet-20240620
    litellm_params:
      model: openai/Qwen/Qwen3-235B-A22B-Instruct  # æŒ‡å‘ vLLM çš„æ¨¡å‹å
      api_base: http://localhost:8000/v1           # vLLM çš„åœ°å€
      api_key: sk-empty                            # vLLM é»˜è®¤ä¸éœ€è¦ keyï¼Œéšä¾¿å¡«
      timeout: 600                                 # é˜²æ­¢å¤§æ¨¡å‹æ€è€ƒå¤ªä¹…è¶…æ—¶

general_settings:
  master_key: sk-1234      # è®¾ç½®ä¸€ä¸ªè¿æ¥ LiteLLM ç”¨çš„å¯†ç 
  
  # === æ ¸å¿ƒé­”æ³• ===
  # å¼€å¯è¿™ä¸ªé€‰é¡¹ï¼ŒLiteLLM ä¼šè‡ªåŠ¨ä¸¢å¼ƒæ‰ vLLM ä¸æ”¯æŒçš„å‚æ•°
  # è¿™æ˜¯è§£å†³ "400 Bad Request" é”™è¯¯çš„å…³é”®ï¼
  drop_params: true
  
  # è°ƒè¯•æ—¥å¿—ï¼Œå¦‚æœå‡ºé”™å¯ä»¥çœ‹åˆ°è¯¦ç»†ä¿¡æ¯
  detailed_debug: true
```

### å¯åŠ¨ LiteLLM ç½‘å…³
åœ¨æ–°çš„ç»ˆç«¯çª—å£ä¸­è¿è¡Œï¼š

```bash
litellm --config litellm_config.yaml --port 4000
```

ç°åœ¨ï¼Œä½ æœ‰ä¸€ä¸ªè¿è¡Œåœ¨ `http://localhost:4000` çš„â€œå‡â€ Claude API äº†ï¼

---

## ğŸ”Œ 5. ç¬¬ä¸‰æ­¥ï¼šè¿æ¥ Claude Code

ç°åœ¨å¯ä»¥é…ç½®ä½ çš„å®¢æˆ·ç«¯äº†ã€‚

### æ–¹å¼ A: åœ¨ç»ˆç«¯ä½¿ç”¨ç¯å¢ƒå˜é‡ (Linux/Mac/WSL)
```bash
export ANTHROPIC_BASE_URL="http://localhost:4000"
export ANTHROPIC_API_KEY="sk-1234"

# å¯åŠ¨ Claude Code
claude-code
```

### æ–¹å¼ B: åœ¨ Cursor / VSCode æ’ä»¶ä¸­é…ç½®
å¦‚æœä½ çš„æ’ä»¶æ”¯æŒè‡ªå®šä¹‰ Base URLï¼š
1.  **Base URL**: `http://localhost:4000` (æ³¨æ„ä¸æ˜¯ 8000ï¼Œæ˜¯ LiteLLM çš„ç«¯å£)
2.  **API Key**: `sk-1234`
3.  **Model Name**: æ‰‹åŠ¨è¾“å…¥ `claude-3-5-sonnet-20240620`

---

## âœ… 6. éªŒè¯æµ‹è¯•

ä¸ºäº†ç¡®ä¿ä¸€åˆ‡æ­£å¸¸ï¼Œæˆ‘ä»¬å†™ä¸€ä¸ª Python è„šæœ¬æ¥æ¨¡æ‹Ÿå·¥å…·è°ƒç”¨ã€‚

åˆ›å»º `test_setup.py`:

```python
import openai

# è¿æ¥åˆ° LiteLLM (ç«¯å£ 4000)
client = openai.OpenAI(
    api_key="sk-1234",
    base_url="http://localhost:4000" 
)

# å®šä¹‰ä¸€ä¸ªæŸ¥è¯¢å¤©æ°”çš„å·¥å…·
tools = [{
    "type": "function",
    "function": {
        "name": "get_weather",
        "description": "è·å–æŸåœ°çš„å¤©æ°”",
        "parameters": {
            "type": "object",
            "properties": {
                "location": {"type": "string", "description": "åŸå¸‚åï¼Œå¦‚ Beijing"}
            },
            "required": ["location"]
        }
    }
}]

print("æ­£åœ¨å‘é€è¯·æ±‚ç»™ Qwen3 (é€šè¿‡ LiteLLM)...")

try:
    response = client.chat.completions.create(
        model="claude-3-5-sonnet-20240620",
        messages=[{"role": "user", "content": "æ­å·ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"}],
        tools=tools
    )
    
    print("\nâœ… è¯·æ±‚æˆåŠŸï¼")
    print("æ¨¡å‹å›å¤å†…å®¹:", response.choices[0].message)
    
    if response.choices[0].message.tool_calls:
        print("\nğŸ‰ æˆåŠŸè§¦å‘å·¥å…·è°ƒç”¨ï¼é…ç½®å®Œç¾ï¼")
    else:
        print("\nâš ï¸ è­¦å‘Šï¼šæ¨¡å‹å›å¤äº†æ–‡æœ¬ï¼Œä½†æ²¡æœ‰è°ƒç”¨å·¥å…·ã€‚å¯èƒ½éœ€è¦ä¼˜åŒ– Prompt æˆ–æ£€æŸ¥ vLLM å‚æ•°ã€‚")

except Exception as e:
    print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
```

è¿è¡Œæµ‹è¯•ï¼š
```bash
python test_setup.py
```

## â“ å¸¸è§é—®é¢˜ (Q&A)

**Q: ä¸ºä»€ä¹ˆ vLLM å¯åŠ¨æ—¶æŠ¥é”™ OOM (Out Of Memory)?**
A: æ˜¾å­˜ä¸è¶³ã€‚å°è¯•å‡å° `--max-model-len` (ä¾‹å¦‚è®¾ä¸º 8192)ï¼Œæˆ–è€…å¢åŠ  GPU æ•°é‡ã€‚

**Q: ä¸ºä»€ä¹ˆ Claude Code è¿˜æ˜¯æŠ¥ 400?**
A: 
1. ç¡®ä¿ä½ è¿æ¥çš„æ˜¯ LiteLLM çš„ç«¯å£ (4000) è€Œä¸æ˜¯ vLLM (8000)ã€‚
2. ç¡®ä¿ `litellm_config.yaml` ä¸­é…ç½®äº† `drop_params: true`ã€‚
3. æ£€æŸ¥ LiteLLM ç»ˆç«¯çš„æŠ¥é”™æ—¥å¿—ï¼Œçœ‹å…·ä½“æ˜¯å“ªä¸ªå‚æ•°ä¸åˆæ³•ã€‚

**Q: Git æ¨é€å¤±è´¥ï¼Ÿ**
A: è¯·æ£€æŸ¥ä½ çš„ç½‘ç»œè¿æ¥ï¼Œæˆ–è€…ç¡®è®¤ä½ æœ‰æƒé™æ¨é€åˆ° `githubstudycloud/gi016` ä»“åº“ã€‚
